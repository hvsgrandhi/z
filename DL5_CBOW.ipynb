{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbd9abb-4a26-42e6-bf71-6437d297d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Lambda, Dense, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3e563d-bc17-43d3-af45-eb9a26e994bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "Climate change refers to significant, long-term changes in the global climate.\n",
    "It is primarily driven by human activities, such as burning fossil fuels, deforestation, and industrial processes,\n",
    "which increase the levels of greenhouse gases in the atmosphere. These gases trap heat, leading to a warming effect known as global warming.\n",
    "Consequences of climate change include more frequent and severe weather events, rising sea levels, and impacts on ecosystems and biodiversity.\n",
    "Efforts to address climate change focus on reducing emissions, transitioning to renewable energy, and enhancing adaptation strategies.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocessing: lowercasing and removing punctuation\n",
    "climate_data = re.sub(r'[^\\w\\s]', '', data.lower()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a115674-4a56-4dd5-b805-be1b282e0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.layers.TextVectorization(split='whitespace')\n",
    "tokenizer.adapt(climate_data)\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "word2id = {word: index for index, word in enumerate(vocab)}\n",
    "id2word = {index: word for word, index in word2id.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69275658-6f41-41d2-8965-8c5072ea8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 67\n",
      "Sample Vocabulary: [('', 0), ('[UNK]', 1), ('and', 2), ('to', 3), ('climate', 4), ('the', 5), ('change', 6), ('warming', 7), ('on', 8), ('of', 9)]\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample Vocabulary:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f267637-4617-4b07-842a-7a7281ad01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for i in range(window_size, len(corpus) - window_size):\n",
    "        context = corpus[i - window_size:i] + corpus[i + 1:i + window_size + 1]\n",
    "        target = corpus[i]\n",
    "        x = pad_sequences([context], maxlen=context_length, padding='post')\n",
    "        y = to_categorical([target], vocab_size)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddcf1d84-0abd-4683-bddc-73d5efedb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ids = [word2id[word] for word in climate_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e39cb5b-cb42-4a7d-beb8-7977dfb94689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            6700      \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 67)                6767      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,467\n",
      "Trainable params: 13,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_size = 100  # Embedding vector size\n",
    "window_size = 2\n",
    "\n",
    "cbow = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size * 2),\n",
    "    Lambda(lambda x: K.mean(x, axis=1)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "cbow.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d12a27-b9fa-4193-8253-8d453c798021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 349.2967734336853\n",
      "Epoch 2/20, Loss: 344.31340503692627\n",
      "Epoch 3/20, Loss: 339.5966351032257\n",
      "Epoch 4/20, Loss: 333.8280704021454\n",
      "Epoch 5/20, Loss: 326.6017556190491\n",
      "Epoch 6/20, Loss: 317.60401368141174\n",
      "Epoch 7/20, Loss: 306.6628601551056\n",
      "Epoch 8/20, Loss: 293.7828891277313\n",
      "Epoch 9/20, Loss: 279.1548116207123\n",
      "Epoch 10/20, Loss: 263.1309566497803\n",
      "Epoch 11/20, Loss: 246.17086362838745\n",
      "Epoch 12/20, Loss: 228.77025842666626\n",
      "Epoch 13/20, Loss: 211.39238679409027\n",
      "Epoch 14/20, Loss: 194.4177689552307\n",
      "Epoch 15/20, Loss: 178.12046813964844\n",
      "Epoch 16/20, Loss: 162.6712628006935\n",
      "Epoch 17/20, Loss: 148.1606655716896\n",
      "Epoch 18/20, Loss: 134.62850654125214\n",
      "Epoch 19/20, Loss: 122.08715283870697\n",
      "Epoch 20/20, Loss: 110.53339275717735\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=corpus_ids, window_size=window_size, vocab_size=vocab_size):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30ef848-bbac-41ce-899b-bda63ffc1d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (67, 100)\n"
     ]
    }
   ],
   "source": [
    "weights = cbow.get_layer('embedding').get_weights()[0]\n",
    "print(\"Embedding Matrix Shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfa9ca2-c83f-4267-a444-b2b9a1d35ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0         1         2         3         4         5         6   \\\n",
      "        -0.001759 -0.049306 -0.007503  0.016459 -0.030443 -0.034881 -0.008138   \n",
      "[UNK]   -0.007659 -0.034708 -0.010460 -0.002799  0.049308 -0.036350 -0.031440   \n",
      "and      0.174661 -0.114852 -0.128286 -0.151958  0.573530 -0.061079 -0.409518   \n",
      "to      -0.267396  0.527209 -0.029663 -0.206797  0.146578 -0.154393 -0.012032   \n",
      "climate -0.217565 -0.438991 -0.042155  0.270349  0.011287  0.456488 -0.034412   \n",
      "\n",
      "               7         8         9   ...        90        91        92  \\\n",
      "         0.034004  0.034712  0.023625  ...  0.026993  0.040169  0.013453   \n",
      "[UNK]   -0.002631  0.015436  0.045613  ...  0.036392 -0.037663  0.045154   \n",
      "and     -0.392264  0.080493 -0.323466  ... -0.270399 -0.077988 -0.020328   \n",
      "to       0.186201  0.020365 -0.325079  ... -0.585742 -0.024113 -0.258423   \n",
      "climate  0.265897  0.063658 -0.324793  ...  0.299212  0.094291 -0.222663   \n",
      "\n",
      "               93        94        95        96        97        98        99  \n",
      "        -0.032240  0.025033 -0.031980 -0.044383  0.020226  0.037596 -0.015563  \n",
      "[UNK]    0.007842 -0.031187  0.003806  0.025689 -0.010916  0.043273  0.030831  \n",
      "and      0.173433  0.270109  0.116281  0.306710 -0.477628  0.100024 -0.209392  \n",
      "to      -0.284106  0.369591  0.416986  0.401999  0.139680  0.104343  0.154482  \n",
      "climate  0.008300  0.008621 -0.322745 -0.234162  0.240821 -0.486213  0.481020  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "embeddings_df = pd.DataFrame(weights, index=[id2word[i] for i in range(vocab_size)])\n",
    "print(embeddings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dde93c2-9515-4600-8abb-608226d1ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = euclidean_distances(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29432b34-9b1f-4c4e-8d56-8e7a27e57816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'climate': ['change', '', 'address', '[UNK]', 'more']\n"
     ]
    }
   ],
   "source": [
    "def get_similar_words(search_term, top_n=5):\n",
    "    term_id = word2id[search_term]\n",
    "    distances = distance_matrix[term_id]\n",
    "    closest_ids = distances.argsort()[1:top_n + 1]\n",
    "    similar_words = [id2word[idx] for idx in closest_ids]\n",
    "    return similar_words\n",
    "\n",
    "print(\"Similar words to 'climate':\", get_similar_words('climate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4d112-4850-4ab4-9622-7d623d1328cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
